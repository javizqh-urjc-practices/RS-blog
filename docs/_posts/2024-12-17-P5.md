---
title: "P5: Marker based visul localization"
date: 2024-12-17 18:00:00 +0100
tags: [weekly progress]     # TAG names should always be lowercase
author: javier
img_path: /assets/img/
toc: true
comments: true
pin: true
---

## Localizate the robot in the world

To localizate the robot in the world we need to solve this equation:

![Matrix](p5-2.png)

I will explained how I obtained these matrixes in the following sections.

### Creating the rotation-translation matrix from the camera to the robot

This one is the simplest one. This matrix can be obtained from the sdf files that define the turtlebot3. In this case the matrix is:

![Matrix](p5-3.png)

### Creating the rotation-translation matrix from the marker to the camera

To obtain this one we will need to follow the next 3 steps:

1. Find in the image all the apriltags
2. Obtain the four corners of each apriltag using the detect function from the pyapriltag package
3. Using the solvePnp function from Opencv with the mode SOLVEPNP_IPPE_SQUARE we get the rotation and translation vectors

Finally with these vector we need to do 2 things:

1. Convert the rotation vector to a 3x3 matrix using the Rodrigues function from Opencv
2. Add padding to the vectors to convert them into 4x4 matrixes.

For the rotation matrix it would be:

![Matrix](p5-4.png)

And for the translation one it would be:

![Matrix](p5-5.png)

Then with the 2 matrixes we multiply them appliying first the rotation followed by the translation and we obtain the matrix from the camera to the marker.

So, finally we invert this matrix to obtain our desired one, the matrix from the marker to the camera.

But this is not the end, the axes of this matrix are not the ones used in Gazebo so we will need to change it for the sake of only using the Gazebo coordinate system.

The one Opencv uses can be defined as: X = right; Y = Downward; Z = Forward relative to the camera. And the one Gazebo uses is defined as: X = East; Y = North; Z = Up.

The neccesary rotations needed are one in the X axis and another one on the Z axis both of -90ยบ.

After we multiply this matrix by the ```mathRT_{MarkerCamera}``` matrix we obtain our final matrix: ```mathRT_{MarkerCamera}``` in the Gazebo coordinate system.

### Creating the rotation-translation matrix from the world to the marker

To obtain this matrix, we need to use the tag id that we can get using the same detect function as the section above.

With that id we search in the given dictionary that contains the position of the tags in the world. Then after obtaining those values we have to repeat the same method as above.

To create the rotation part we use the third value found in the position of the tag that indicates the angle the marker is rotated in the Z axis. The final rotation matrix will look like this (3x3, we will add padding to it as done above):

![Matrix](p5-6.png)

To create the translation counterpart we use the first 2 values that indicate the X and Y axes, and for the Z axis we use the given tag height that is 0.8 meters. The final translation matrix will look like this (3x1, we will add padding to it as done above):

![Matrix](p5-7.png)

Then with the 2 matrixes we multiply them appliying first the rotation followed by the translation and we obtain the matrix from the world to the marker.

### Creating the rotation-translation matrix from the world to the robot

For this one we solve the equation:

![Matrix](p5-2.png)

Using `np.matmul()` to do the matrix multiplication, and we obtain our robot position in the world

## Basic robot movement

## Videos
