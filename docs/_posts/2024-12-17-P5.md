---
title: "P5: Marker based visul localization"
date: 2024-12-17 18:00:00 +0100
tags: [weekly progress]     # TAG names should always be lowercase
author: javier
img_path: /assets/img/
toc: true
comments: true
pin: true
---

## Localizate the robot in the world

To localizate the robot in the world we need to solve this equation:

![Matrix](p5-2.png)

I will explained how I obtained these matrixes in the following sections.

Then with the resulting matrix we need to extract the X, Y and yaw values. For the X and Y values it is simple, because those values can be extracted from the translation part, being the first one the X, then Y and lastly Z.

For the yaw it is a bit more difficult. To get it first we need to obtain the pitch and we can do that solving the next equation:

```python
pitch = atan2(-R[2, 0], sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0]))
```

Then with that we finally get the yaw solving another simple equation:

```python
yaw = atan2(R[1, 0] / cos(pitch), R[0, 0] / cos(pitch))
```

With this we know everything about the robot position: X, Y and yaw.

### Multiple Tags

If there are multiple tags in the image we need to weigh each value to get the best possible approximation. To do that I used the method explained in this [paper](https://gsyc.urjc.es/jmplaza/papers/jornadasautomatica2016-pnp.pdf).

### Creating the rotation-translation matrix from the camera to the robot

This one is the simplest one. This matrix can be obtained from the sdf files that define the turtlebot3. In this case the matrix is:

![Matrix](p5-3.png)

### Creating the rotation-translation matrix from the marker to the camera

To obtain this one we will need to follow the next 3 steps:

1. Find in the image all the apriltags
2. Obtain the four corners of each apriltag using the detect function from the pyapriltag package
3. Using the solvePnp function from Opencv with the mode SOLVEPNP_IPPE_SQUARE we get the rotation and translation vectors

Finally with these vector we need to do 2 things:

1. Convert the rotation vector to a 3x3 matrix using the Rodrigues function from Opencv
2. Add padding to the vectors to convert them into 4x4 matrixes.

For the rotation matrix it would be:

![Matrix](p5-4.png)

And for the translation one it would be:

![Matrix](p5-5.png)

Then with the 2 matrixes we multiply them appliying first the rotation followed by the translation and we obtain the matrix from the camera to the marker.

So, finally we invert this matrix to obtain our desired one, the matrix from the marker to the camera.

But this is not the end, the axes of this matrix are not the ones used in Gazebo so we will need to change it for the sake of only using the Gazebo coordinate system.

The one Opencv uses can be defined as: X = right; Y = Downward; Z = Forward relative to the camera. And the one Gazebo uses is defined as: X = East; Y = North; Z = Up.

The neccesary rotations needed are one in the X axis and another one on the Z axis both of -90ยบ.

After we multiply this matrix by the ```mathRT_{MarkerCamera}``` matrix we obtain our final matrix: ```mathRT_{MarkerCamera}``` in the Gazebo coordinate system.

### Creating the rotation-translation matrix from the world to the marker

To obtain this matrix, we need to use the tag id that we can get using the same detect function as the section above.

With that id we search in the given dictionary that contains the position of the tags in the world. Then after obtaining those values we have to repeat the same method as above.

To create the rotation part we use the third value found in the position of the tag that indicates the angle the marker is rotated in the Z axis. The final rotation matrix will look like this (3x3, we will add padding to it as done above):

![Matrix](p5-6.png)

To create the translation counterpart we use the first 2 values that indicate the X and Y axes, and for the Z axis we use the given tag height that is 0.8 meters. The final translation matrix will look like this (3x1, we will add padding to it as done above):

![Matrix](p5-7.png)

Then with the 2 matrixes we multiply them appliying first the rotation followed by the translation and we obtain the matrix from the world to the marker.

### Creating the rotation-translation matrix from the world to the robot

For this one we solve the equation:

![Matrix](p5-2.png)

Using `np.matmul()` to do the matrix multiplication, and we obtain our robot position in the world

## Localizate the robot in the world with no tags

When there are no tags visible we need to continue updating the estimated robot position and to do that we just need to get the difference between our old position and the new one.

In order to get the new one we are provided with the getOdom function that gives the postion based in the odometry. We are only allowed to use this function to get how much the robot has moved in the last iteration.

After getting said movement we just update the old position with this new one.

## Basic robot movement

To move the robot in a random way I decided to do a navigation based in going forward until it crashes with an obstacle and then rotate a random number of seconds, between 3 and 5.

When no tags are found we slow the robot because we are not sure of the position in the map, and also to be ready for the impact with a lower speed.

To do the obstacle detection I first assume that when there is an obstacle no tags are visible, and second I use the movement obtained in the section above this one.

If said movement has been less than 1 mm in both axes in the last 20 iterations, it is not needed to check if the angle changes because the only time it rotates it uses a timer, we assume that there is an obstacle in front of the robot.

When there is one we rotate the robot the random time mentioned above and also go backwards a little bit in order to not get stuck in a wall (the robot is square, not a circle).

This rotation stops when we find a new tag or the random time passes.

## Things to note

As can be seen in the videos below the localization using tags is a bit inconsistent if the distance between the tag an the camera is greater than 4 meters and can cause the estimated position to be with a lot of error. Also when the robot loses visibility of the tag sometimes it will estimate the position wrongly and then the robot position will have that error until another tag shows up.

Lastly, sometimes the obstacle detection in the movement may trigger because of the change in speed when losing the tag visibility. This will be easily solved if there were availables a laser or a bumper so we could base the obstacle detection on those.

## Videos

{% include embed/youtube.html id='-K1K6Wkfx58' %}

{% include embed/youtube.html id='rdQZJGbAimE' %}
